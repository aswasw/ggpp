{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6486"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the json FIle contain tweets and there labes \n",
    "#For train and test\n",
    "from pyspark.sql import SparkSession\n",
    "jobDir = \"tweets1.json\" # tain data\n",
    "tweets = spark.read.json([jobDir])\n",
    "tweets.count() #number of tweets in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the object in json file\n",
    "tweets = tweets.select(\"text\", \\\n",
    "                     \"Category\" )\n",
    "\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer,CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#convert a collection of text documents to vectors of token counts. \n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer\n",
    "#StringIndexer encodes a string column of labels to a column of label indices.\n",
    "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=MulticlassClassificationEvaluator(), \\\n",
    "                    numFolds=5)\n",
    "pipeline = Pipeline(stages=[regexTokenizer,label_stringIdx, countVectors,cv])\n",
    "model2 = pipeline.fit(tweets) # use the train data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"vvvyes3\")# save the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Ingestion and Extraction\n",
    "from pyspark.sql import SparkSession\n",
    "jobDir = \"tweetsJson.json\" # new data without label\n",
    "test = spark.read.json([jobDir])\n",
    "test.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the object in json file\n",
    "test = test.select(\"text\")\n",
    "\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model2 = PipelineModel.load(\"vvvyes3\")# use the model to classify the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.transform(test)# get the predictions of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+----------+\n",
      "|text                                                                                 |prediction|\n",
      "+-------------------------------------------------------------------------------------+----------+\n",
      "|ÙƒÙ„ ÙŠÙˆÙ… ØªØ§ÙƒØ¯ Ø§Ù† Ø¬ÙˆØ¯ Ø§Ù… Ù Ø­ÙŠØ§Øª Ø§Ù‡Ù… Ø¬ÙˆØ¯ÙŠ Ø´Ø®ØµÙŠ â¤ï¸                                        |1.0       |\n",
      "|ÙŠØ¹ Ø§Ù†ØªÙŠ Ø§Ù… Ù…ÙØªÙƒØ±ØªÙŠØ´ Ø§Ù†Ø§ Ø§Ø¨Ù‚ÙŠ Ø§Ø¨Ù† Ù…ÙŠÙ† -Ø§Øº Ø§ØªÙˆØ¨ÙŠØ³ 2019                                 |1.0       |\n",
      "| Ø§Ù†Ø§ Ø§Ù… Ù…Ø§Øª Ø¯Ù‡Ø³ ÙˆØ§Ù„Ø¬ ÙŠØªÙ…ØªØ¹ Ø¨Ø­ÙŠ Ù‡Ø§Ù†Ø¡ Ù…Ø§ØªÙˆÙ‚Ù ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯ Ø­Ø±Ù‚ Ù‚Ù„ÙˆØ¨ Ø§Ù„Ù„Ù‡ ÙŠØ­Ø±Ù‚ Ù‚Ù„Ø¨ ğŸ’”ğŸ˜­      |0.0       |\n",
      "|Ø§Ù… Ø¯Ø§Ø² Ù…Ù‚Ø·Ø¹ ØªÙ‚ÙˆÙ„ Ø­ØªØ§ Ù‚Ø±Ø¯ Ø±Ø§ÙŠØ­ Ù…Ø³Ø¬Ø¯ ÙŠØ³Ù…Ø¹ Ø®Ø·Ø¨Ù‡ Ùˆ Ù†Ø§ÙŠÙ… Ø¨ÙŠØª                              |0.0       |\n",
      "|Ø§Ù… Ù…ØµÙˆØ± Ø²ÙŠ Ø²Ù‚Øª Ø·Ø§Ù„Ø¹ ØµÙˆØ±Ù‡ ğŸ˜­ğŸ˜­                                                        |0.0       |\n",
      "|Ø§Ù… ØªØµÙ†Ø¹ Ù‚Ù„Ø¨ Ø¨ÙŠÙˆ Ø§Ù…Ù„ ØªØµÙ„Ø­ Ø§ÙØ³Ø¯ Ø­ÙŠØ§ ØªØ±Ù…Ø±Ù… Ø®Ø±Ø§Ø¨ Ø±ÙˆØ­ ØªØ³Ø¯ Ø«Ù‚Ø¨ Ø¬Ø±Ø­ ØªÙ…Ø­ Ø§Ù„Ù… ØµØ¯Ø±  ØªØ³Ø¹Ù‰â€¦      |2.0       |\n",
      "| Ø§Ù…                                                                                  |0.0       |\n",
      "| Ø§Ù… â™¥ï¸                                                                               |2.0       |\n",
      "|Ø§Ù„Ù„ ÙŠØ±Ø­Ù…                                                                             |0.0       |\n",
      "|Ø§Ù„Ù„ ÙŠØ´Ù ÙŠØ®Ù„                                                                          |0.0       |\n",
      "| Ø§Ù†Ø³ ÙŠØ¹Ø¯Ù„ Ù‚Ø±Ø¨ Ù…Ù†Ù‡ ØªÙ„Ø°Ø° Ø­Ø¯ÙŠØ« Ø±ÙŠØ­ Ø²Ø§Ùƒ ÙƒØ±ÙŠÙ… Ø¬ÙˆØ¯  Ø¯Ø¹Ø§Ø¡ ØµØ§Ù Ø§Ù†Ù‡ Ø­ÙŠØ§Ù‡ Ø®â€¦                   |1.0       |\n",
      "|Ø¨Ø±Ø­Ù… Ø§Ø±Ø­Ù… Ø±Ø§Ø­Ù… Ø§Ø±Ø­Ù… Ø§Ù… Ù†ÙˆØ± ÙˆØ§ØºÙØ± ÙˆØ³Ø¹ Ù‚Ø¨Ø± Ø§Ù†Ø³ ÙˆØ­Ø´ ÙˆØ§Ø¬Ù…Ø¹ Ø¬Ù†Øª ÙˆØ§Ø±Ø­Ù… Ù…ÙˆØªÙŠ Ù…Ø³Ù„Ù… ÙŠØ§Ø±Ø¨ Ø¹Ø§Ù„Ù… |2.0       |\n",
      "|Ø§Ù„Ù„ Ø§Ø¬Ø¹Ù„ Ø§Ù… ÙˆØ§Ø¨ Ø³Ø¨Ø¹ Ø§Ù„Ù ÙŠØ¯Ø®Ù„ Ø¬Ù†Ù‡ Ø¨Ù„Ø§ Ø­Ø³Ø§Ø¨ Ø³Ø§Ø¨Ù‚ Ø¹Ø°Ø§Ø¨â¤ï¸                                |0.0       |\n",
      "|Ù…Ø¯Ø± Ø¬Ù„Ø³ Ø¨Ø·Ù† Ø§Ù… 9 Ø´Ù‡ÙˆØ± Ø¯ÙˆÙ† Ø¬ÙˆØ§Ù„ ÙˆÙ†Øª                                                   |0.0       |\n",
      "|Ø³ØªØ¨Ù‚ÙŠ Ø§Ù… Ø¯Ø§ÙŠÙ… Ø´ÙŠØ¡ Ù…Ø®ØªÙ„Ù Ø¬Ù…ÙŠØ¹â¤ï¸                                                       |2.0       |\n",
      "|Ø§Ù… Ù†ÙˆØ± Ø¨Ù‚Ø§Ø¡ Ù„Ù„Ù‡ ÙˆÙ…Ùˆ Ù‚Ø¯Ø± ğŸ’”ğŸ’”                                                         |1.0       |\n",
      "|Ø±Ø¨ Ø§ÙˆØµ Ø¨ Ø§Ù… Ø®ÙŠØ± Ø¹Ø§Ù Ø¹Ù…Ø± Ø·ÙˆÙŠÙ„Ø§â™¥ï¸                                                      |1.0       |\n",
      "|Ø³ØªØ¨Ù‚ÙŠ Ø§Ù… Ø¯Ø§ÙŠÙ… Ø´ÙŠØ¡ Ù…Ø®ØªÙ„Ù Ø¬Ù…ÙŠØ¹ğŸ’œ                                                       |2.0       |\n",
      "|Ø§Ù„Ù„ Ø§Ù… Ø¯Ø§ÙŠÙ… Ø§Ø¨Ø¯Ø§ğŸ’™                                                                   |0.0       |\n",
      "|Ø®Ø§Ù„Ø§Øª Ù†Ø¹ÙŠÙ… Ù…Ù‚ØªØ¨Ø³ Ù‚Ù„Ø¨ Ø§Ù… ğŸ’—                                                           |0.0       |\n",
      "+-------------------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"text\",  \"prediction\").show(20,False) # display the predictions of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
