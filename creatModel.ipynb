{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6486"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the json FIle contain tweets and there labes \n",
    "#For train and test\n",
    "from pyspark.sql import SparkSession\n",
    "jobDir = \"tweets1.json\" # tain data\n",
    "tweets = spark.read.json([jobDir])\n",
    "tweets.count() #number of tweets in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the object in json file\n",
    "tweets = tweets.select(\"text\", \\\n",
    "                     \"Category\" )\n",
    "\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer,CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#convert a collection of text documents to vectors of token counts. \n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer\n",
    "#StringIndexer encodes a string column of labels to a column of label indices.\n",
    "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=MulticlassClassificationEvaluator(), \\\n",
    "                    numFolds=5)\n",
    "pipeline = Pipeline(stages=[regexTokenizer,label_stringIdx, countVectors,cv])\n",
    "model2 = pipeline.fit(tweets) # use the train data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"vvvyes3\")# save the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Ingestion and Extraction\n",
    "from pyspark.sql import SparkSession\n",
    "jobDir = \"tweetsJson.json\" # new data without label\n",
    "test = spark.read.json([jobDir])\n",
    "test.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the object in json file\n",
    "test = test.select(\"text\")\n",
    "\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model2 = PipelineModel.load(\"vvvyes3\")# use the model to classify the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.transform(test)# get the predictions of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+----------+\n",
      "|text                                                                                 |prediction|\n",
      "+-------------------------------------------------------------------------------------+----------+\n",
      "|كل يوم تاكد ان جود ام ف حيات اهم جودي شخصي ❤️                                        |1.0       |\n",
      "|يع انتي ام مفتكرتيش انا ابقي ابن مين -اغ اتوبيس 2019                                 |1.0       |\n",
      "| انا ام مات دهس والج يتمتع بحي هانء ماتوقف يوم واحد حرق قلوب الله يحرق قلب 💔😭      |0.0       |\n",
      "|ام داز مقطع تقول حتا قرد رايح مسجد يسمع خطبه و نايم بيت                              |0.0       |\n",
      "|ام مصور زي زقت طالع صوره 😭😭                                                        |0.0       |\n",
      "|ام تصنع قلب بيو امل تصلح افسد حيا ترمرم خراب روح تسد ثقب جرح تمح الم صدر  تسعى…      |2.0       |\n",
      "| ام                                                                                  |0.0       |\n",
      "| ام ♥️                                                                               |2.0       |\n",
      "|الل يرحم                                                                             |0.0       |\n",
      "|الل يشف يخل                                                                          |0.0       |\n",
      "| انس يعدل قرب منه تلذذ حديث ريح زاك كريم جود  دعاء صاف انه حياه خ…                   |1.0       |\n",
      "|برحم ارحم راحم ارحم ام نور واغفر وسع قبر انس وحش واجمع جنت وارحم موتي مسلم يارب عالم |2.0       |\n",
      "|الل اجعل ام واب سبع الف يدخل جنه بلا حساب سابق عذاب❤️                                |0.0       |\n",
      "|مدر جلس بطن ام 9 شهور دون جوال ونت                                                   |0.0       |\n",
      "|ستبقي ام دايم شيء مختلف جميع❤️                                                       |2.0       |\n",
      "|ام نور بقاء لله ومو قدر 💔💔                                                         |1.0       |\n",
      "|رب اوص ب ام خير عاف عمر طويلا♥️                                                      |1.0       |\n",
      "|ستبقي ام دايم شيء مختلف جميع💜                                                       |2.0       |\n",
      "|الل ام دايم ابدا💙                                                                   |0.0       |\n",
      "|خالات نعيم مقتبس قلب ام 💗                                                           |0.0       |\n",
      "+-------------------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"text\",  \"prediction\").show(20,False) # display the predictions of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
