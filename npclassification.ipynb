{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7992"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Ingestion and Extraction\n",
    "from pyspark.sql import SparkSession\n",
    "jobDir = \"tweets1.json\"\n",
    "tweets = spark.read.json([jobDir])\n",
    "tweets.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.select(\"text\", \\\n",
    "                     \"Category\" )\n",
    "\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+------------------------------+-----+\n",
      "|                          text|Category|                         words|                      features|label|\n",
      "+------------------------------+--------+------------------------------+------------------------------+-----+\n",
      "|والل عجب عشان كتاب انجليز ص...|     POS|[والل, عجب, عشان, كتاب, انج...|(17072,[68,109,147,351,644,...|  2.0|\n",
      "|انه مفيد جدان انا اتعلم كثي...|     POS|[انه, مفيد, جدان, انا, اتعل...|(17072,[19,22,126,3800,9802...|  2.0|\n",
      "|انه رنامج رايع يترجم كلم قط...|     POS|[انه, رنامج, رايع, يترجم, ك...|(17072,[19,147,242,251,1087...|  2.0|\n",
      "|رنامج جميل جدا يترجم كلم جم...|     POS|[رنامج, جميل, جدا, يترجم, ك...|(17072,[82,127,147,166,242,...|  2.0|\n",
      "|قمه روعه الف شكر تقدير مجهو...|     POS|[قمه, روعه, الف, شكر, تقدير...|(17072,[82,97,103,398,635,1...|  2.0|\n",
      "|      جميل اشخاص ايجد انجلزيه |     POS|  [جميل, اشخاص, ايجد, انجلزيه]|(17072,[166,881,14206,16489...|  2.0|\n",
      "|          عاش ايد برنامج رايع |     POS|      [عاش, ايد, برنامج, رايع]|(17072,[251,332,459,756],[1...|  2.0|\n",
      "|      برنامج حلوو وانصح تحميل |     POS|  [برنامج, حلوو, وانصح, تحميل]|(17072,[459,3115,11912,1524...|  2.0|\n",
      "|     حلو والل صرت احل واجب ال |     POS|[حلو, والل, صرت, احل, واجب,...|(17072,[14,68,262,917,3873,...|  2.0|\n",
      "|              برنامج جدا جميل |     POS|           [برنامج, جدا, جميل]|(17072,[82,166,459],[1.0,1....|  2.0|\n",
      "+------------------------------+--------+------------------------------+------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, countVectors, label_stringIdx])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(tweets)\n",
    "dataset = pipelineFit.transform(tweets)\n",
    "dataset.show(10,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 5654\n",
      "Test Dataset Count: 2335\n"
     ]
    }
   ],
   "source": [
    "#Partition Training & Test sets\n",
    "# set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+--------+--------------------------------------------------------------+-----+----------+\n",
      "|                                                                                                text|Category|                                                   probability|label|prediction|\n",
      "+----------------------------------------------------------------------------------------------------+--------+--------------------------------------------------------------+-----+----------+\n",
      "|مضحك صفح اسلاميه جري ورا كلام استاذه مافيش اصل كتاب تاليف اسماعيل الام اسم عرب يغز اندلسشكلكو وحش...|     NEG|  [0.4952558690488377,0.5003486842382693,0.004395446712893042]|  0.0|       1.0|\n",
      "|                     قصد مو ضروره خالف طيف معنا افضليه لك مخالف الهم تكون حال دخول طيف اخر استدع ا… | NEUTRAL| [0.49439186988298844,0.5000805562303184,0.005527573886693169]|  1.0|       1.0|\n",
      "| ملاحق باسم يوسف زملاء اعلام تهم تعرف الا انظمه فاشيه استمرار ممارسا قبيح بايس اجهاض ثوره تغيير حتم |     NEG|  [0.4929905985257966,0.5013582090913029,0.005651192382900559]|  0.0|       1.0|\n",
      "|     اخو اختار رضا امريك خلاص بان زمان وده الل هيعمق خلاف سلف ومش عيد تلاق يعاير سلف علاق بعض بالام |     NEG|[0.48647107311460847,0.5123391760225439,0.0011897508628475988]|  0.0|       1.0|\n",
      "|   يعتد خالد داود صايح تاع جبه انقاذ افرفليصم زياد هاء دين رفاق حديث مصالحه الا سنرد علي حديث اصابع |     NEG| [0.48148689001834394,0.5181276737508561,3.854362307998589E-4]|  0.0|       1.0|\n",
      "|                                                                       مرس غيرش نايب عام ليه دي نهض |     NEG| [0.48141331115317676,0.5143159569329399,0.004270731913883364]|  0.0|       1.0|\n",
      "|                   قضاي وطن عام دايم اسعي بناء توافق حول جميع انضمام عمل تحت مظل اخو غير مرفوض تمام | NEUTRAL|  [0.4756643495820263,0.5139567797381746,0.010378870679799089]|  1.0|       1.0|\n",
      "|                ومش عيد خالص تكون الشء معنويه الل باع اساس وهم غباء معتاد وقع مصيده تردد هيموت غباء |     NEG|   [0.475605362168779,0.5239553583818111,4.392794494098267E-4]|  0.0|       1.0|\n",
      "|                         وهذ يعن حركه صهيونيه قبل نشوف دول كان فكر تمش ارض منظم وله اموال عقول مكان | NEUTRAL|  [0.4735552508659783,0.48231052398717844,0.04413422514684337]|  1.0|       1.0|\n",
      "|                   د عبد منعم ابو فتوح يشرفني يسعد اول عيد ثوره ابعث تحياتي تهنيتي لاسر شهداء هولاء |     POS| [0.46804919915107507,0.5263371105697846,0.005613690279140355]|  2.0|       1.0|\n",
      "+----------------------------------------------------------------------------------------------------+--------+--------------------------------------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NaiveBayes Count Vector Features\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"text\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6642816494568803"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                          text|Category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|جميل ان يكتشف حسن نوا تجا ظ...| NEUTRAL|[0.4977393044568139,0.50226...|  1.0|       1.0|\n",
      "|اسكت وله بلا هبل شو جاب دوا...|     NEG|[0.4875593706290305,0.51244...|  0.0|       1.0|\n",
      "| رطم حني مليان زياد ماش يدلدق |     POS|[0.472567594696991,0.527432...|  2.0|       1.0|\n",
      "|عاجل مياد نت معلوما موكد عم...| NEUTRAL|[0.4691478048626397,0.53085...|  1.0|       1.0|\n",
      "|يعن يعقل ان ينتخب شعب مصر ر...| NEUTRAL|[0.4560640377961834,0.54393...|  1.0|       1.0|\n",
      "|الي ارد معر وجه حق دبر مذبح...| NEUTRAL|[0.44985451214263983,0.5501...|  1.0|       1.0|\n",
      "|عالم يتج تقليل منتظم مستمر ...|     NEG|[0.4483661588382104,0.55163...|  0.0|       1.0|\n",
      "|كلم هيب محتاج تتشال قاموس ع...|     NEG|[0.4360785502762834,0.56392...|  0.0|       1.0|\n",
      "|لء حساب يبحث محلل او كاتب م...| NEUTRAL|[0.4241555636487789,0.57584...|  1.0|       1.0|\n",
      "|قضاي ابتزاز سعوديه سبب مقاط...|     NEG|[0.40261192573751653,0.5973...|  0.0|       1.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NaiveBayes using TF-IDF Features\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(tweets)\n",
    "dataset = pipelineFit.transform(tweets)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"text\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266986748408847"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6642816494568803"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-Validation\n",
    "pipeline = Pipeline(stages=[regexTokenizer,countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(tweets)\n",
    "dataset = pipelineFit.transform(tweets)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [1, 3, 5])\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=nb, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = predictions.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|        1085|\n",
      "|  1.0|         600|\n",
      "|  2.0|         650|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pl.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|             1171|\n",
      "|       1.0|              515|\n",
      "|       2.0|              649|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pl.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 66.767%\n"
     ]
    }
   ],
   "source": [
    "acc = pl.filter(pl.label == pl.prediction).count() / pl.count()\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
