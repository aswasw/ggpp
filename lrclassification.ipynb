{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7988"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the json FIle contain tweets and there labes \n",
    "#For train and test\n",
    "from pyspark.sql import SparkSession\n",
    "jobDir = \"tweets1.json\"\n",
    "tweets = spark.read.json([jobDir])\n",
    "tweets.count() #number of tweets in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select the object in json file\n",
    "tweets = tweets.select(\"text\", \\\n",
    "                     \"Category\" )\n",
    "\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer,CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "#convert a collection of text documents to vectors of token counts. \n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----+----------+\n",
      "|      text|Category|     words|label|  features|\n",
      "+----------+--------+----------+-----+----------+\n",
      "|ÙˆØ§Ù„Ù„ Ø¹Ø¬...|     POS|[ÙˆØ§Ù„Ù„, ...|  2.0|(17070,...|\n",
      "|Ø§Ù†Ù‡ Ø±Ù†Ø§...|     POS|[Ø§Ù†Ù‡, Ø±...|  2.0|(17070,...|\n",
      "|Ø±Ù†Ø§Ù…Ø¬ Ø¬...|     POS|[Ø±Ù†Ø§Ù…Ø¬,...|  2.0|(17070,...|\n",
      "|Ù‚Ù…Ù‡ Ø±ÙˆØ¹...|     POS|[Ù‚Ù…Ù‡, Ø±...|  2.0|(17070,...|\n",
      "|Ø¬Ù…ÙŠÙ„ Ø§Ø´...|     POS|[Ø¬Ù…ÙŠÙ„, ...|  2.0|(17070,...|\n",
      "|Ø¹Ø§Ø´ Ø§ÙŠØ¯...|     POS|[Ø¹Ø§Ø´, Ø§...|  2.0|(17070,...|\n",
      "|Ø¨Ø±Ù†Ø§Ù…Ø¬ ...|     POS|[Ø¨Ø±Ù†Ø§Ù…Ø¬...|  2.0|(17070,...|\n",
      "|Ø­Ù„Ùˆ ÙˆØ§Ù„...|     POS|[Ø­Ù„Ùˆ, Ùˆ...|  2.0|(17070,...|\n",
      "|Ø¨Ø±Ù†Ø§Ù…Ø¬ ...|     POS|[Ø¨Ø±Ù†Ø§Ù…Ø¬...|  2.0|(17070,...|\n",
      "|Ø±Ø§ÙŠØ¹ Ø¬Ø¯...|     POS|[Ø±Ø§ÙŠØ¹, ...|  2.0|(17070,...|\n",
      "+----------+--------+----------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer\n",
    "#StringIndexer encodes a string column of labels to a column of label indices.\n",
    "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "#pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, label_stringIdx, countVectors])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(tweets)\n",
    "dataset = pipelineFit.transform(tweets)\n",
    "dataset.show(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 6394\n",
      "Test Dataset Count: 1594\n"
     ]
    }
   ],
   "source": [
    "#Partition Training & Test sets\n",
    "#80% train ,20% test\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-----+----------+\n",
      "|      text|Category|probability|label|prediction|\n",
      "+----------+--------+-----------+-----+----------+\n",
      "|ğŸ˜‚ Ø³Ù„Ø§Ø­...|     NEG| [0.9996...|  0.0|       0.0|\n",
      "|Ø§ÙƒØ± ÙˆØ§Ø­...|     NEG| [0.9898...|  0.0|       0.0|\n",
      "|Ù‡Ø¯ÙˆÙ„ Ù…Ø§...|     POS| [0.9876...|  2.0|       0.0|\n",
      "|Ø§Ø³Ø®Ù Ù…Ù†...|     NEG| [0.9719...|  0.0|       0.0|\n",
      "|ØªØ¹Ù„Ù… Ø§Ù†...|     NEG| [0.9698...|  0.0|       0.0|\n",
      "|ØªØ¹Ù„Ù… Ø§Ù†...|     NEG| [0.9698...|  0.0|       0.0|\n",
      "|Ø§Ø³ØªØºØ±Ø¨ ...|     NEG| [0.9681...|  0.0|       0.0|\n",
      "|Ù…ØªØ¹Ø¨ Ø¹Ù†...|     NEG| [0.9670...|  0.0|       0.0|\n",
      "|â€œØ¨Ø·Ù„ ØªÙ...|     NEG| [0.9649...|  0.0|       0.0|\n",
      "|Ø³Øª Ø²ÙŠÙ†Ø¨...|     NEG| [0.9625...|  0.0|       0.0|\n",
      "+----------+--------+-----------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression using Count Vector Features\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)# model will make predictions and score on the test set\n",
    "\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.6743941550173962\n"
     ]
    }
   ],
   "source": [
    "# Show the accuracy \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"f1\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the label from our dataset\n",
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|         728|\n",
      "|  1.0|         412|\n",
      "|  2.0|         454|\n",
      "+-----+------------+\n",
      "\n",
      "the label from test\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|              943|\n",
      "|       1.0|              289|\n",
      "|       2.0|              362|\n",
      "+----------+-----------------+\n",
      "\n",
      "Model accuracy: 68.256%\n"
     ]
    }
   ],
   "source": [
    "pl = predictions.select(\"label\", \"prediction\")\n",
    "\n",
    "print(\"the label from our dataset\") \n",
    "pl.groupby('label').agg({'label': 'count'}).show()\n",
    "\n",
    "print(\"the label from test\") \n",
    "pl.groupby('prediction').agg({'prediction': 'count'}).show()\n",
    "\n",
    "pl.filter(pl.label == pl.prediction).count() / pl.count()\n",
    "acc = pl.filter(pl.label == pl.prediction).count() / pl.count()\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+--------+---------------------------------------------------------------+-----+----------+\n",
      "|                                                                                                text|Category|                                                    probability|label|prediction|\n",
      "+----------------------------------------------------------------------------------------------------+--------+---------------------------------------------------------------+-----+----------+\n",
      "|ğŸ˜‚ Ø³Ù„Ø§Ø­ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø¯Ù… Ø¯Ø³ØªÙˆØ± Ù„ÙŠØ´ Ø§Ù„ÙƒÙ„ Ø¹Ù… ÙŠÙ‚ÙˆÙ„ Ø§Ù†Ùˆ Ù‚Ø¯Ù… Ø§Ø³ØªÙ‚Ø§Ù„ Ø®Ø§Ø±Ø¬Ù…Ø§ Ø§ÙŠ Ø¯Ù„ÙŠÙ„ Ø§â€¦ Ø³Ù„Ø§Ø­ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ù‚Ø¯Ù… Ø¯Ø³ØªÙˆØ± ...|     NEG|[0.9998595001194233,1.1238710338103115E-4,2.811277719581919E-5]|  0.0|       0.0|\n",
      "|ÙŠÙ‚ÙˆÙ„ ØªØ·ÙˆØ±Ø§ Ø®Ø·ÙŠØ± Ù…Ù†Ø·Ù‚Ù‡ ÙˆÙƒØ¡ Ø§ÙƒØ«Ø±Ù… Ù…Ù„ÙŠÙˆÙ† Ø´Ù‡ÙŠØ¯ Ø¹Ø±Ø§Ù‚ Ø³ÙˆØ± Ø¹Ø´Ø±Ø§ Ø§Ù„ÙˆÙ ÙŠÙ…Ù† Ù„ÙŠØ¨ÙŠ Ù„Ø³Ø· ØªØ¯Ù…ÙŠØ± Ø­Ù„Ø¨ Ù…ÙˆØµÙ„ ØªØ·ÙˆØ±Ø§ Ø³...|     NEG|  [0.9883705526327038,0.00477643783829598,0.006853009529000155]|  0.0|       0.0|\n",
      "|Ø§ÙŠØ§ Ø¹Ù…Ø± ÙØ§Ø±ÙˆÙ‚ Ø¹ÙˆØ¯ Ø¬ÙŠÙˆØ´ ÙØ±Ø³ ØªÙ†Ù‡ÙŠ ØªØ§Ù…Ø± ÙŠØ­Ø§ØµØ± Ù…ÙˆØª Ù…Ù„ÙŠÙˆÙ† ÙƒØ§ÙØ± ÙÙ Ø´Ø±Ù‚ Ù‡ÙˆÙ„Ø§Ùƒ ÙˆÙ ØºØ±Ø¨ Ù‚ÙŠØµâ€¦ Ù† Ø¬ÙŠÙˆØ´ ÙØ±Ø³ ØªÙ†Ù‡...|     POS|  [0.988283335013959,0.005724997482042065,0.005991667503998998]|  2.0|       0.0|\n",
      "|Ù‡Ø¯ÙˆÙ„ Ù…Ø§Ø¨ØºÙ„Ø¨ Ø­Ø§Ù„ ÙŠÙ‡Ù…ğŸ˜‚ Ø§Ù„Ù„ ØµØ§Ø¯Ù ÙŠÙˆÙ… Ø³Ø¨Ø¨ Ø§Ù†Ù‡ Ù„ÙŠØ¯ Ø­Ø¯ Ù…Ù†ÙŠØ­ Ù‡Ù„Ø§Ø§Ù„Ù„ ØµØ§Ø¯Ù ÙŠÙˆÙ… Ø³Ø¨Ø¨ Ø§Ù†Ù‡ Ù„ÙŠØ¯ Ø­Ø¯ Ù…Ù†ÙŠØ­ Ù„ÙŠØ¯ Ù‡Ø¯...|     POS|  [0.9871290189378876,0.010058109659404594,0.00281287140270777]|  2.0|       0.0|\n",
      "|ØªØ¹Ù„Ù… Ø§Ù† Ø­Ø§Ø±Ø³ Ù…Ù†ØªØ®Ø¨ Ø³ÙˆØ± Ø­Ø§Ù„ Ø´Ø¨ÙŠØ­ Ø§Ø®Ùˆ Ø´Ø±Ù…ÙˆØ· ÙƒØ§Ù† ÙŠØ­Ù…Ù„ Ø³Ù„Ø§Ø­ ÙŠÙ‚ØªÙ„ Ø³ÙˆØ± Ø§Ø¨Ø±ÙŠØ§Ø¡ ÙˆØ§Ù„Ù„ Ø´ÙƒÙ„ Ù…Ø§â€¦ Ø³Ù„Ø§Ø­ ÙŠÙ‚ØªÙ„ Ø³Ùˆ...|     NEG| [0.9847176894603463,0.007333137994364951,0.007949172545288858]|  0.0|       0.0|\n",
      "|ØªØ¹Ù„Ù… Ø§Ù† Ø­Ø§Ø±Ø³ Ù…Ù†ØªØ®Ø¨ Ø³ÙˆØ± Ø­Ø§Ù„ Ø´Ø¨ÙŠØ­ Ø§Ø®Ùˆ Ø´Ø±Ù…ÙˆØ· ÙƒØ§Ù† ÙŠØ­Ù…Ù„ Ø³Ù„Ø§Ø­ ÙŠÙ‚ØªÙ„ Ø³ÙˆØ± Ø§Ø¨Ø±ÙŠØ§Ø¡ ÙˆØ§Ù„Ù„ Ø´ÙƒÙ„ Ù…Ø§â€¦ Ø³Ù„Ø§Ø­ ÙŠÙ‚ØªÙ„ Ø³Ùˆ...|     NEG| [0.9847176894603463,0.007333137994364951,0.007949172545288858]|  0.0|       0.0|\n",
      "|Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø§Ø­Ø¯ Ù…ØµÙˆØ± ØªÙ„ÙØ² Ø¹Ø±Ø¨ Ø³ÙˆØ± Ù…Ø­Ù…Ø¯ Ù…ÙŠÙ„Ø§Ø¯ Ø§Ø«Ø± Ø§Ù†ÙØ¬Ø§Ø± Ù„ØºÙ… Ù…Ø®Ù„ÙØ§ ØªÙ†Ø¸ÙŠÙ… Ø¯Ø§Ø¹Ø´ Ø±ÙŠÙ Ø­Ù…Øµ Ø´Ø±Ù‚ â–ª Ø§Ø­Ø¯ Ù…ØµÙˆØ± Øª...|     NEG|  [0.9760004628242998,0.01583592899357575,0.008163608182124478]|  0.0|       0.0|\n",
      "|                          Ø¹Ø§Ø± Ø¹Ø§Ø± Ø¹Ø§Ø± Ø¹Ø§Ø± Ø¹Ø§Ø± Ø¹Ø§Ø± Ù…Ø§Ù†Ø­ Ø¹Ø§Ø± Ø¯Ù…Ø§Ø¡ Ø§Ù†Ù‡Ø§Ø± Ø´Ù‡Ø¯Ø§Ø¡ Ø¹Ø´Ø± Ø±ØµØ§Øµ ÙÙŠ Ø±Ø§Ø³ Ø±Ù‚Ø¨ ØµØ¯Ø± |     NEG| [0.9703106116421547,0.016551583497030054,0.013137804860815226]|  0.0|       0.0|\n",
      "|                Ù†Ù†Ø§Ù‚Ø´ Ù‚Ø±Ø§Ø± Ø­ÙƒÙˆÙ…Ù‡ Ù†ØªØ­ÙŠØ² Ø¨Ø­Ø¬ Ù„Ø§Ø¯ Ø¨Ø±Ù‡ ÙƒÙ„Ù‡ ØªÙ†Ø§Ù… Ø¯Ø±ÙŠØªÙŠØ¬ Ø§Ø·Ù„Ø§Ù‚ Ø­Ø±ÙŠ Ø¹Ù‚ÙŠØ¯Ù‡ Ø§Ø­Ù†Ø§ Ù…Ø§Ù„ Ø¨Ù„Ø§Ø¯ Ø¨Ø± | NEUTRAL|   [0.967621765360928,0.011401237930435331,0.02097699670863655]|  1.0|       0.0|\n",
      "|       Ø­Ø±ÙŠØ± Ø¹Ù…Ù„ Ù…Ù‡Ø¶ÙˆÙ… Ù‚Ø§Ù… Ù†Ø²Ù„ Ø¹Ø§Ù„Ù…Ø¸Ø§Ù‡Ø±Ø§ Ø±Ø¯Ùˆ Ù‚Ù†Ø§ Ù…ÙŠ Ø§Ø°Ø§ Ø±Ø¯ ØµÙŠØ± Ø¹Ø§Ø·Ù„ Ù‡ÙŠØ¯ Ø³Ø¹Ø¯ Ø­Ø±ÙŠØ± ÙŠÙ„ Ù…Ø´ Ø¹Ø§Ø¬Ø¨ ÙŠØ¨Ù„Ø· Ø¨Ø­Ø± |     NEG|  [0.9642337244824772,0.015079031275022468,0.02068724424250022]|  0.0|       0.0|\n",
      "+----------------------------------------------------------------------------------------------------+--------+---------------------------------------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression using TF-IDF Features\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(tweets)\n",
    "dataset = pipelineFit.transform(tweets)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.6444852381016305\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",metricName=\"f1\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the label from our dataset\n",
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|         728|\n",
      "|  1.0|         412|\n",
      "|  2.0|         454|\n",
      "+-----+------------+\n",
      "\n",
      "the label from test\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|              928|\n",
      "|       1.0|              316|\n",
      "|       2.0|              350|\n",
      "+----------+-----------------+\n",
      "\n",
      "Model accuracy: 65.307%\n"
     ]
    }
   ],
   "source": [
    "pl = predictions.select(\"label\", \"prediction\")\n",
    "\n",
    "print(\"the label from our dataset\") \n",
    "pl.groupby('label').agg({'label': 'count'}).show()\n",
    "\n",
    "print(\"the label from test\") \n",
    "pl.groupby('prediction').agg({'prediction': 'count'}).show()\n",
    "\n",
    "pl.filter(pl.label == pl.prediction).count() / pl.count()\n",
    "acc = pl.filter(pl.label == pl.prediction).count() / pl.count()\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "pipeline = Pipeline(stages=[regexTokenizer,countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(tweets)\n",
    "dataset = pipelineFit.transform(tweets)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "pipeline = Pipeline(stages=[regexTokenizer,countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(tweets)\n",
    "dataset = pipelineFit.transform(tweets)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, [0.1, 0.3, 0.5]) \\\n",
    "  .addGrid(lr.maxIter, [10, 20, 50]) \\\n",
    "  .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) \\\n",
    "  .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.6782760772380454\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the label from our dataset\n",
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|         728|\n",
      "|  1.0|         412|\n",
      "|  2.0|         454|\n",
      "+-----+------------+\n",
      "\n",
      "the label from test\n",
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|              882|\n",
      "|       1.0|              328|\n",
      "|       2.0|              384|\n",
      "+----------+-----------------+\n",
      "\n",
      "Model accuracy: 68.319%\n"
     ]
    }
   ],
   "source": [
    "pl = predictions.select(\"label\", \"prediction\")\n",
    "\n",
    "print(\"the label from our dataset\") \n",
    "pl.groupby('label').agg({'label': 'count'}).show()\n",
    "\n",
    "print(\"the label from test\") \n",
    "pl.groupby('prediction').agg({'prediction': 'count'}).show()\n",
    "\n",
    "pl.filter(pl.label == pl.prediction).count() / pl.count()\n",
    "acc = pl.filter(pl.label == pl.prediction).count() / pl.count()\n",
    "print(\"Model accuracy: %.3f%%\" % (acc * 100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.save(\"myModelPath4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.save(\"vModel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import  CrossValidatorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameCVModel = CrossValidatorModel.load(\"vModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
